import json
from statistics import mean
from pathlib import Path
from pprint import pprint


INPUT_DIRECTORY = Path("/input")
OUTPUT_DIRECTORY = Path("/output")
GROUND_TRUTH_DIRECTORY = Path("/tmp/ground_truth")

{% if "__should_fail" in cookiecutter.phase -%} 1/0 {%- endif %}

def main():
    print_inputs()

    metrics = {"case": {}}
    predictions = read_predictions()

    for job in predictions:
        pass
        # We now iterate over each algorithm job for this submission
        # Note that the jobs are not in any order!
        # We work that out from predictions.json

        # This corresponds to one archive item in the archive
{#        cfp_stack_filename = get_image_name(#}
{#            values=job["inputs"], slug="stacked-color-fundus-images"#}
{#        )#}
{#        # Parse one of the filename to get the batch ID#}
{#        batch_id = cfp_stack_filename.split(".")[0]#}
{##}
{#        pprint(f"Processing batch {batch_id}")#}
{##}
{#        # Now we can get the locations of users inference output for this archive item#}
{#        is_referable_glaucoma_stacked_location = get_file_location(#}
{#            job_pk=job["pk"],#}
{#            values=job["outputs"],#}
{#            slug="stacked-referable-glaucoma-binary-decision",#}
{#        )#}
{#        referable_glaucomatous_features_stacked_location = get_file_location(#}
{#            job_pk=job["pk"],#}
{#            values=job["outputs"],#}
{#            slug="stacked-referable-glaucomatous-features",#}
{#        )#}
{##}
{#        # Now we load those files to get their content#}
{#        is_referable_glaucoma_stacked = load_json_file(#}
{#            location=is_referable_glaucoma_stacked_location#}
{#        )#}
{#        referable_glaucomatous_features_stacked = load_json_file(#}
{#            location=referable_glaucomatous_features_stacked_location#}
{#        )#}
{##}
{#        # Now you would need to load your ground truth, include it in the evaluation container#}
{##}
{#        # Now we need to loop over the stack content and process it against#}
{#        # any ground ruths#}
{#        for item in current_stack_info:#}
{#            index = item["stack_index"]#}
{##}
{#            # The follow three data points should be sufficient to map to your groundthruth#}
{#            original_image_filename = item["image"]#}
{#            predicted_referable_glaucoma = is_referable_glaucoma_stacked[index]#}
{#            predicted_glaucomatous_features = referable_glaucomatous_features_stacked[#}
{#                index#}
{#            ]#}

        # And here you need to compare the predictions with the ground truth and generate a score for this case
        # For now, perfect scores:
{#        metrics["case"][batch_id] = {"my_metric": 1}#}
{##}
{#        print("")#}

{#    # Now generate an overall score#}
{#    metrics["aggregates"] = {#}
{#        "my_metric": mean(batch["my_metric"] for batch in metrics["case"].values())#}
{#    }#}

    write_metrics(metrics=metrics)

    return 0

def process_prediction_jog(job):
    pass

def print_inputs():
    # Just for convenience, in the logs you can then see what files you have to work with
    input_files = [str(x) for x in Path(INPUT_DIRECTORY).rglob("*") if x.is_file()]

    print("Input Files:")
    pprint(input_files)
    print("")


def read_predictions():
    # The prediction file tells us the location of the users' predictions
    with open(f"{INPUT_DIRECTORY}/predictions.json") as f:
        return json.loads(f.read())


def get_image_name(*, values, slug):
    # This tells us the user-provided name of the input or output image
    for value in values:
        if value["interface"]["slug"] == slug:
            return value["image"]["name"]

    raise RuntimeError(f"Image with interface {slug} not found!")


def get_interface_relative_path(*, values, slug):
    # Gets the location of the interface relative to the input or output
    for value in values:
        if value["interface"]["slug"] == slug:
            return value["interface"]["relative_path"]

    raise RuntimeError(f"Value with interface {slug} not found!")


def get_file_location(*, job_pk, values, slug):
    # Where a job's output file will be located in the evaluation container
    relative_path = get_interface_relative_path(values=values, slug=slug)
    return f"{INPUT_DIRECTORY}/{job_pk}/output/{relative_path}"


def load_json_file(*, location):
    # Reads a json file
    with open(location) as f:
        return json.loads(f.read())


def write_metrics(*, metrics):
    # Write a json document used for ranking results on the leaderboard
    with open("/output/metrics.json", "w") as f:
        f.write(json.dumps(metrics))


if __name__ == "__main__":
    raise SystemExit(main())
